<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>sentspace API documentation</title>
<meta name="description" content="Sentspace 0.0.2 (C) 2020-2022 [EvLab](evlab.mit.edu), MIT BCS. All rights reserved …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>sentspace</code></h1>
</header>
<section id="section-intro">
<h3 id="sentspace-002-c-2020-2022-evlab-mit-bcs-all-rights-reserved">Sentspace 0.0.2 (C) 2020-2022 <a href="evlab.mit.edu">EvLab</a>, MIT BCS. All rights reserved.</h3>
<p>Homepage: <a href="https://sentspace.github.io/sentspace">https://sentspace.github.io/sentspace</a></p>
<p>For questions, email:</p>
<p><code>{gretatu,asathe} @ mit.edu</code></p>
<h1 id="sentspace">sentspace</h1>
<p><img src="graphics/logo_v1.png" width="300"></p>
<!-- ABOUT THE PROJECT -->
<h2 id="about">About</h2>
<p><code><a title="sentspace" href="#sentspace">sentspace</a></code> is an open-source tool for characterizing text using diverse features related to how humans process and understand language.
<code><a title="sentspace" href="#sentspace">sentspace</a></code> characterizes textual input using cognitively motivated lexical, syntactic, and semantic features computed at the token- and sentence level. Features are derived from psycholinguistic experiments, large-scale corpora, and theoretically motivated models of language processing.
The <code><a title="sentspace" href="#sentspace">sentspace</a></code> features fall into two core modules: Lexical and Contextual. The Lexical module operates on individual lexical items (words) within a sentence and computes a summary representation by combining information across the words in the sentence. This module includes features such as frequency, concreteness, age of acquisition, lexical decision latency, contextual diversity, etc.
The Contextual module operates on sentences as whole and includes syntactic features, such as the depth of center embedding. Note that using the contextual module requires additional set up steps (see in the setup section below). </p>
<p>New modules can be easily added to SentSpace to provide additional ways to characterize text.
In this manner, <code><a title="sentspace" href="#sentspace">sentspace</a></code> provides a quantitative and interpretable representation of any sentence. </p>
<p><strong>GitHub repository:</strong> <a href="http://github.com/sentspace/sentspace">http://github.com/sentspace/sentspace</a></p>
<p><strong>Screencast video demo:</strong> <a href="https://youtu.be/a66_nvcCakw">https://youtu.be/a66_nvcCakw</a></p>
<p><strong>CLI usage demo:</strong></p>
<!-- ![image](https://i.imgur.com/lI6Wose.gif) -->
<p><img src="https://i.imgur.com/lI6Wose.gif" alt="drawing" width="800"/></p>
<h2 id="documentation"><a href="https://sentspace.github.io/sentspace">Documentation</a></h2>
<!-- [![CircleCI](https://circleci.com/gh/aalok-sathe/sentspace/tree/main.svg?style=svg)](https://circleci.com/gh/aalok-sathe/sentspace/tree/main) -->
<!-- request read access to the [project doc](https://docs.google.com/document/d/1O1M7T5Ji6KKRvDfI7KQXe_LJ7l9O6_OZA7TEaVP4f8E/edit#). -->
<p>Documentation is available online (click on the title above).</p>
<h2 id="usage">Usage</h2>
<h3 id="1-cli">1. CLI</h3>
<p>Example: get lexical and embedding features for stimuli from a csv containing columns for 'sentence' and 'index'.</p>
<pre><code class="language-bash">$ python3 -m sentspace -h
usage: 


positional arguments:
  input_file            path to input file or a single sentence. If supplying a file, it must be .csv .txt or .xlsx, e.g., example/example.csv

optional arguments:
  -h, --help            show this help message and exit
  -sw STOP_WORDS, --stop_words STOP_WORDS
                        path to delimited file of words to filter out from analysis, e.g., example/stopwords.txt
  -b BENCHMARK, --benchmark BENCHMARK
                        path to csv file of benchmark corpora For example benchmarks/lexical/UD_corpora_lex_features_sents_all.csv
  -p PARALLELIZE, --parallelize PARALLELIZE
                        use multiple threads to compute features? disable using `-p False` in case issues arise.
  -o OUTPUT_DIR, --output_dir OUTPUT_DIR
                        path to output directory where results may be stored
  -of {pkl,tsv}, --output_format {pkl,tsv}
  -lex LEXICAL, --lexical LEXICAL
                        compute lexical features? [False]
  -con CONTEXTUAL, --contextual CONTEXTUAL
                        compute syntactic features? [False]
  --emb_data_dir EMB_DATA_DIR
                        path to output directory where results may be stored
</code></pre>
<h3 id="2-as-a-library">2. As a library</h3>
<p>Example: get embedding features in a script</p>
<pre><code class="language-python">import sentspace

s = sentspace.Sentence('The person purchased two mugs at the price of one.')
emb_features = sentspace.embedding.get_features(s)
</code></pre>
<p>Example: parallelize getting features for multiple sentences using multithreading</p>
<pre><code class="language-python">import sentspace

sentences = [
    'Hello, how may I help you today?',
    'The person purchased three mugs at the price of five!',
    'This is an example sentence we want features of.'
             ]

# construct sentspace.Sentence objects from strings
sentences = [*map(sentspace.Sentence, sentences)]
# make use of parallel processing to get lexical features for the sentences
lex_features = sentspace.utils.parallelize(sentspace.lexical.get_features, sentences,
                                           wrap_tqdm=True, desc='Computing lexical features')
</code></pre>
<h2 id="installing">Installing</h2>
<h3 id="1-install-using-uv">1. Install using <a href="https://github.com/astral-sh/uv">uv</a></h3>
<ol>
<li>Install <code>uv</code>: refer to the link above.</li>
<li>Install <code>pyicu</code>: Installation instruction varies by your OS, and this package lives outside of the python package system so you'll need to manually install it</li>
<li><code>ubuntu</code>: pre-built package is available via <code>apt</code>: <code>sudo apt-get install python3-icu</code></li>
<li><code>macOS</code>: see <a href="https://pypi.org/project/pyicu/">here</a></li>
<li>Install <code><a title="sentspace" href="#sentspace">sentspace</a></code> and its dependencies using <code>uv</code> and the <code>uv.lock</code> lockfile already present in this repo: <code>uv sync --extra polyglot</code> (recommended: install <code>polyglot</code> to support additional optional features such as accurate morphological segmentation/lemmatization)</li>
<li>Activate the virtual env: <code>. ./.venv/bin/activate</code>
<br></li>
</ol>
<h3 id="2-container-based-usage">2. Container-based usage</h3>
<p><a href="https://circleci.com/gh/aalok-sathe/sentspace/tree/circle-ci"><img alt="CircleCI" src="https://circleci.com/gh/aalok-sathe/sentspace/tree/circle-ci.svg?style=svg"></a></p>
<p>Requirements: <code>singularity</code> or <code>docker</code>. </p>
<!-- #### **first, some important housekeeping stuff**
- make sure you have <code>singularity</code>/<code>docker</code>, or load/install it otherwise
- <code>which singularity</code>
or
<code>which docker</code>
- make sure you have set the ennvironment variables that specify where `singularity/docker` will cache its images. if you don't do this, <code>singularity</code> will make assumptions and you may end up with a full disk and an unresponsive server, if running on a server with filesystem restrictions. you should have about 5GB free space at the target location. -->
<!-- #### **next, running the container** (automatically built and deployed to Docker hub) -->
<p><strong>Singularity:</strong></p>
<pre><code class="language-bash">singularity shell docker://aloxatel/sentspace:latest
</code></pre>
<p>Alternatively, from the root of the repo, <code>bash singularity-shell.sh</code>). this step can take a while when you run it for the first time as it needs to download the image from docker hub and convert it to singularity image format (<code>.sif</code>). however, each subsequent run will execute rapidly. </p>
<p><strong>Docker:</strong> use <a href="https://docs.docker.com/engine/reference/commandline/exec/">corresponding commands for Docker</a>.</p>
<p>now you are inside the container and ready to run <code><a title="sentspace" href="#sentspace">sentspace</a></code>!</p>
<h3 id="3-manual-install-use-as-last-resort">3. Manual install (use as last resort)</h3>
<p>On Debian/Ubuntu-like systems, follow the steps below. On other systems (RHEL, etc.),
substitute commands and package names with appropriate alternates.</p>
<pre><code class="language-bash"># optional (but recommended): 
# create a virtual environment using your favorite method (venv, conda, ...) 
# before any of the following

# install basic packages using apt (you likely already have these)
sudo apt update
sudo apt install python3.8 python3.8-dev python3-pip
sudo apt install python2.7 python2.7-dev 
sudo apt install build-essential git

# install ICU
DEBIAN_FRONTEND=&quot;noninteractive&quot; TZ=&quot;America/New_York&quot; sudo apt install python3-icu

# install ZS package separately (pypi install fails)
python3.8 -m pip install -U pip cython
git clone https://github.com/njsmith/zs
cd zs &amp;&amp; git checkout v0.10.0 &amp;&amp; pip install .

# install rest of the requirements using pip
cd .. # make sure you're in the sentspace/ directory
pip install -r ./requirements.txt
polyglot download morph2.en
</code></pre>
<h2 id="submodules">Submodules</h2>
<p>SentSpace features fall into two core modules: Lexical and Contextual.
In general, each submodule implements a major class of features.
You can run each module on its own by specifying its flag and <code>0</code> or <code>1</code> with the module call:</p>
<pre><code class="language-bash">python -m sentspace -lex {0,1} -con {0,1} &lt;input_file_path&gt;
</code></pre>
<p>For a full list of available features, refer to the Feature Descriptions page <a href="https://sentspace.github.io/hosted">on the hosted SentSpace frontend</a>.</p>
<h4 id="sentspacelexical"><code><a title="sentspace.lexical" href="lexical/index.html">sentspace.lexical</a></code></h4>
<p>The Lexical module consists of features that pertain to individual lexical items, words, regardless of the context in which they appear.
These features are returned on a word-by-word level and also aggregated at the sentence level to provide each sentence a corresponding value.</p>
<h4 id="contextual"><code>contextual</code></h4>
<p>The Contextual module consists of features that quantify contextual and combinatorial inter-word
relations that are not captured by individual lexical items. This module encompasses features that
relate to the syntactic structure of the sentence (<code>Contextual_syntax</code> features) and features that
apply to the sentence context but are not (exclusively) related to syntactic structure
(<code>Contextual_misc</code> features).</p>
<p><strong>⚠ Additional steps to set up the contextual module</strong>
The core sentspace program doesn't include a syntax server. It therefore needs to query a backend where PCFG processing can happen.
You'll need to separately run this backend simultaneously to sentspace so that sentspace can query it and obtain features.
The module should be running in a terminal for the duration you're using sentspace, and then you can kill it using Ctrl+C.
- Here's a link to the module: <a href="https://github.com/sentspace/sentspace-syntax-server">https://github.com/sentspace/sentspace-syntax-server</a>
- Jump to the "Setup" section in the readme to run it: <a href="https://github.com/sentspace/sentspace-syntax-server?tab=readme-ov-file#setup-how-to-get-it-up-and-running">https://github.com/sentspace/sentspace-syntax-server?tab=readme-ov-file#setup-how-to-get-it-up-and-running</a>
- There is a pre-built docker image so that this setup should only need 1 command (sudo docker run -it &ndash;net=host &ndash;expose 8000 -p 8000:8000 aloxatel/berkeleyparser:latest). There is also a corresponding <code>singularity</code> command for HPC cluster environs that works with the same docker image.
- This will start loading and eventually [it can take 5-10 minutes, it is slow] expose the syntax server on port 8000 (this is just a virtual address so other processes on your computer know where to look!)
- Now, sentspace can query your localhost port 8000 with sentences to be processed, and it will be returned syntax-based features for further post-processing and packaging into a nice output format similar to rest of sentspace.
- To make sure it knows to talk to the correct port, you can either pass it into the CLI (&ndash;syntax_port 8000) or as an argument to the library function: <a href="https://github.com/sentspace/sentspace/blob/4b0f79c7f6dcab6285d3af42101b04b05f421b01/sentspace/__main__.py#L127">https://github.com/sentspace/sentspace/blob/4b0f79c7f6dcab6285d3af42101b04b05f421b01/sentspace/__main__.py#L127</a>
However, both, the library and the syntax server should default to <code>localhost:8000</code> so unless you have a special circumstance, you won't need to worry about this.</p>
<!--
#### <code>embedding</code>
Obtain high dimensional representations of sentences using word-embedding and contextualized encoder models.
- <code>glove</code>
- Huggingface model hub (`gpt2-xl`, `bert-base-uncased`)
#### <code>semantic</code>
Multi-word features computed using partial or full sentence context.
- PMI (pointwise mutual information)
- Language model-based perplexity/surprisal
*Not Implemented yet*
-->
<h2 id="contributing">Contributing</h2>
<p>Any contributions you make are <strong>greatly appreciated</strong>, and no contribution is <em>too small</em> to contribute.</p>
<ol>
<li>Fork the project on Github <a href="https://docs.github.com/en/get-started/quickstart/fork-a-repo">(how to fork)</a></li>
<li>Create your feature/patch branch (<code>git checkout -b feature/AmazingFeature</code>)</li>
<li>Commit your changes (<code>git commit -m 'Add some AmazingFeature'</code>)</li>
<li>Push the branch (<code>git push origin feature/AmazingFeature</code>)</li>
<li>Open a Pull Request (PR) and we will take a look asap!</li>
</ol>
<h2 id="whom-to-contact-for-help">Whom to contact for help</h2>
<ul>
<li><code>gretatu % mit ^ edu</code></li>
<li><code>asathe % mit ^ edu</code></li>
</ul>
<p>(C) 2020-2022 EvLab, MIT BCS</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
### Sentspace 0.0.2 (C) 2020-2022 [EvLab](evlab.mit.edu), MIT BCS. All rights reserved.

Homepage: https://sentspace.github.io/sentspace

For questions, email:

`{gretatu,asathe} @ mit.edu`

.. include:: ../README.md
&#34;&#34;&#34;

from pathlib import Path

import sentspace.utils as utils
import sentspace.syntax as syntax
import sentspace.lexical as lexical
# import sentspace.embedding as embedding

from sentspace.Sentence import Sentence

import pandas as pd
from functools import reduce
from itertools import chain
from tqdm import tqdm


def run_sentence_features_pipeline(
    input_file: str,
    stop_words_file: str = None,
    benchmark_file: str = None,
    output_dir: str = None,
    output_format: str = None,
    batch_size: int = 2_000,
    process_lexical: bool = False,
    process_syntax: bool = False,
    process_embedding: bool = False,
    process_semantic: bool = False,
    parallelize: bool = True,
    # preserve_metadata: bool = True,
    syntax_server: str = &#34;http://localhost/&#34;,
    syntax_port: int = 8000,
    limit: float = float(&#34;inf&#34;),
    offset: int = 0,
    emb_data_dir: str = None,
) -&gt; Path:
    &#34;&#34;&#34;
    Runs the full sentence features pipeline on the given input according to
    requested submodules (currently supported: `lexical`, `syntax`, `embedding`,
    indicated by boolean flags).

    Returns an instance of `Path` pointing to the output directory resulting from this
    run of the full pipeline. The output directory contains Pickled or TSVed pandas
    DataFrames containing the requested features.


    Args:
        input_file (str): path to input text file containing sentences
                            one per line [required]
        stop_words_file (str): path to text file containing stopwords to filter
                                out, one per line [optional]
        benchmark_file (str): path to a file containing a benchmark corpus to
                                compare the current input against; e.g. UD [optional]

        {lexical,syntax,embedding,semantic,...} (bool): compute submodule features? [False]
    &#34;&#34;&#34;

    # lock = multiprocessing.Manager().Lock()

    # create output folder
    utils.io.log(&#34;creating output folder&#34;)
    output_dir = utils.io.create_output_paths(
        input_file, output_dir=output_dir, stop_words_file=stop_words_file
    )
    # config_out = output_dir / &#34;this_session_log.txt&#34;
    # with config_out.open(&#39;a+&#39;) as f:
    #     print(args, file=f)

    utils.io.log(&#34;reading input sentences&#34;)
    sentences = utils.io.read_sentences(input_file, stop_words_file=stop_words_file)
    utils.io.log(&#34;---done--- reading input sentences&#34;)

    for part, sentence_batch in enumerate(
        tqdm(
            utils.io.get_batches(
                sentences, batch_size=batch_size, limit=limit, offset=offset
            ),
            desc=&#34;processing batches&#34;,
            total=len(sentences) // batch_size + 1,
        )
    ):
        sentence_features_filestem = f&#34;sentence-features_part{part:0&gt;4}&#34;
        token_features_filestem = f&#34;token-features_part{part:0&gt;4}&#34;

        ################################################################################
        #### LEXICAL FEATURES ##########################################################
        ################################################################################
        if process_lexical:
            utils.io.log(&#34;*** running lexical submodule pipeline&#34;)
            _ = lexical.utils.load_databases(features=&#34;all&#34;)

            lexical_features = utils.parallelize(
                lexical.get_features,
                sentence_batch,
                wrap_tqdm=True,
                desc=&#34;Lexical pipeline&#34;,
                max_workers=None if parallelize else 1,
            )

            lexical_out = output_dir / &#34;lexical&#34;
            lexical_out.mkdir(parents=True, exist_ok=True)
            utils.io.log(f&#34;outputting lexical token dataframe to {lexical_out}&#34;)

            # lexical is a special case since it returns dicts per token (rather than per sentence)
            # so we want to flatten it so that pandas creates a sensible dataframe from it.
            token_df = pd.DataFrame(chain.from_iterable(lexical_features))

            if output_format == &#34;tsv&#34;:
                token_df.to_csv(
                    lexical_out / f&#34;{token_features_filestem}.tsv&#34;, sep=&#34;\t&#34;, index=True
                )
                token_df.groupby(&#34;sentence&#34;).mean().to_csv(
                    lexical_out / f&#34;{sentence_features_filestem}.tsv&#34;,
                    sep=&#34;\t&#34;,
                    index=True,
                )
            elif output_format == &#34;pkl&#34;:
                token_df.to_pickle(
                    lexical_out / f&#34;{token_features_filestem}.pkl.gz&#34;, protocol=5
                )
                token_df.groupby(&#34;sentence&#34;).mean(numeric_only=True).to_pickle(
                    lexical_out / f&#34;{sentence_features_filestem}.pkl.gz&#34;, protocol=5
                )
            else:
                raise ValueError(f&#34;output format {output_format} not known&#34;)

            utils.io.log(&#34;--- finished lexical pipeline&#34;)

        ################################################################################
        #### SYNTAX FEATURES ###########################################################
        ################################################################################
        if process_syntax:
            utils.io.log(&#34;*** running syntax submodule pipeline&#34;)

            syntax_features = [
                syntax.get_features(
                    sentence,
                    dlt=True,
                    left_corner=True,
                    syntax_server=syntax_server,
                    syntax_port=syntax_port,
                )
                for i, sentence in enumerate(
                    tqdm(sentence_batch, desc=&#34;Syntax pipeline&#34;)
                )
            ]

            # put all features in the sentence df except the token-level ones
            token_syntax_features = {&#34;dlt&#34;, &#34;leftcorner&#34;}
            sentence_df = pd.DataFrame(
                [
                    {
                        k: v
                        for k, v in feature_dict.items()
                        if k not in token_syntax_features
                    }
                    for feature_dict in syntax_features
                ],
                index=[s.uid for s in sentence_batch],
            )

            # output gives us dataframes corresponding to each token-level feature. we need to combine these
            # into a single dataframe
            # we use functools.reduce to apply the pd.concat function to all the dataframes and join dataframes
            # that contain different features for the same tokens
            token_dfs = [
                reduce(
                    lambda x, y: pd.concat([x, y], axis=1, sort=False),
                    (v for k, v in feature_dict.items() if k in token_syntax_features),
                )
                for feature_dict in syntax_features
            ]

            for i, df in enumerate(token_dfs):
                token_dfs[i][&#34;index&#34;] = df.index
            #     token_dfs[i].reset_index(inplace=True)

            dicts = [
                {k: v[list(v.keys())[0]] for k, v in df.to_dict().items()}
                for df in token_dfs
            ]
            token_df = pd.DataFrame(dicts)
            token_df.index = token_df[&#34;index&#34;]
            # by this point we have merged dataframes with tokens along a column (rather than just a sentence)
            # now we need to stack them on top of each other to have all tokens across all sentences in a single dataframe
            # token_df = reduce(lambda x, y: pd.concat([x.reset_index(drop=True), y.reset_index(drop=True)]), token_dfs)
            # token_df = token_df.loc[:, ~token_df.columns.duplicated()]

            syntax_out = output_dir / &#34;syntax&#34;
            syntax_out.mkdir(parents=True, exist_ok=True)
            utils.io.log(f&#34;outputting syntax dataframes to {syntax_out}&#34;)

            if output_format == &#34;tsv&#34;:
                sentence_df.to_csv(
                    syntax_out / f&#34;{sentence_features_filestem}.tsv&#34;,
                    sep=&#34;\t&#34;,
                    index=True,
                )
                token_df.to_csv(
                    syntax_out / f&#34;{token_features_filestem}.tsv&#34;, sep=&#34;\t&#34;, index=True
                )
            elif output_format == &#34;pkl&#34;:
                sentence_df.to_pickle(
                    syntax_out / f&#34;{sentence_features_filestem}.pkl.gz&#34;, protocol=5
                )
                token_df.to_pickle(
                    syntax_out / f&#34;{token_features_filestem}.pkl.gz&#34;, protocol=5
                )
            else:
                raise ValueError(f&#34;unknown output format {output_format}&#34;)

            utils.io.log(&#34;--- finished syntax pipeline&#34;)

        # Calculate PMI
        # utils.GrabNGrams(sent_rows,pmi_paths)
        # utils.pPMI(sent_rows, pmi_paths)

        # Plot input data to benchmark data
        # utils.plot_usr_input_against_benchmark_dist_plots(df_benchmark, sent_embed)

    ################################################################################
    #### \end{run_sentence_features_pipeline} ######################################
    ################################################################################
    return output_dir</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="sentspace.Sentence" href="Sentence.html">sentspace.Sentence</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sentspace.lexical" href="lexical/index.html">sentspace.lexical</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sentspace.package_lexical" href="package_lexical.html">sentspace.package_lexical</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sentspace.syntax" href="syntax/index.html">sentspace.syntax</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sentspace.utils" href="utils/index.html">sentspace.utils</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sentspace.vis" href="vis/index.html">sentspace.vis</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sentspace.run_sentence_features_pipeline"><code class="name flex">
<span>def <span class="ident">run_sentence_features_pipeline</span></span>(<span>input_file: str, stop_words_file: str = None, benchmark_file: str = None, output_dir: str = None, output_format: str = None, batch_size: int = 2000, process_lexical: bool = False, process_syntax: bool = False, process_embedding: bool = False, process_semantic: bool = False, parallelize: bool = True, syntax_server: str = 'http://localhost/', syntax_port: int = 8000, limit: float = inf, offset: int = 0, emb_data_dir: str = None) ‑> pathlib.Path</span>
</code></dt>
<dd>
<div class="desc"><p>Runs the full sentence features pipeline on the given input according to
requested submodules (currently supported: <code><a title="sentspace.lexical" href="lexical/index.html">sentspace.lexical</a></code>, <code><a title="sentspace.syntax" href="syntax/index.html">sentspace.syntax</a></code>, <code>embedding</code>,
indicated by boolean flags).</p>
<p>Returns an instance of <code>Path</code> pointing to the output directory resulting from this
run of the full pipeline. The output directory contains Pickled or TSVed pandas
DataFrames containing the requested features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_file</code></strong> :&ensp;<code>str</code></dt>
<dd>path to input text file containing sentences
one per line [required]</dd>
<dt><strong><code>stop_words_file</code></strong> :&ensp;<code>str</code></dt>
<dd>path to text file containing stopwords to filter
out, one per line [optional]</dd>
<dt><strong><code>benchmark_file</code></strong> :&ensp;<code>str</code></dt>
<dd>path to a file containing a benchmark corpus to
compare the current input against; e.g. UD [optional]</dd>
</dl>
<p>{lexical,syntax,embedding,semantic,&hellip;} (bool): compute submodule features? [False]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_sentence_features_pipeline(
    input_file: str,
    stop_words_file: str = None,
    benchmark_file: str = None,
    output_dir: str = None,
    output_format: str = None,
    batch_size: int = 2_000,
    process_lexical: bool = False,
    process_syntax: bool = False,
    process_embedding: bool = False,
    process_semantic: bool = False,
    parallelize: bool = True,
    # preserve_metadata: bool = True,
    syntax_server: str = &#34;http://localhost/&#34;,
    syntax_port: int = 8000,
    limit: float = float(&#34;inf&#34;),
    offset: int = 0,
    emb_data_dir: str = None,
) -&gt; Path:
    &#34;&#34;&#34;
    Runs the full sentence features pipeline on the given input according to
    requested submodules (currently supported: `lexical`, `syntax`, `embedding`,
    indicated by boolean flags).

    Returns an instance of `Path` pointing to the output directory resulting from this
    run of the full pipeline. The output directory contains Pickled or TSVed pandas
    DataFrames containing the requested features.


    Args:
        input_file (str): path to input text file containing sentences
                            one per line [required]
        stop_words_file (str): path to text file containing stopwords to filter
                                out, one per line [optional]
        benchmark_file (str): path to a file containing a benchmark corpus to
                                compare the current input against; e.g. UD [optional]

        {lexical,syntax,embedding,semantic,...} (bool): compute submodule features? [False]
    &#34;&#34;&#34;

    # lock = multiprocessing.Manager().Lock()

    # create output folder
    utils.io.log(&#34;creating output folder&#34;)
    output_dir = utils.io.create_output_paths(
        input_file, output_dir=output_dir, stop_words_file=stop_words_file
    )
    # config_out = output_dir / &#34;this_session_log.txt&#34;
    # with config_out.open(&#39;a+&#39;) as f:
    #     print(args, file=f)

    utils.io.log(&#34;reading input sentences&#34;)
    sentences = utils.io.read_sentences(input_file, stop_words_file=stop_words_file)
    utils.io.log(&#34;---done--- reading input sentences&#34;)

    for part, sentence_batch in enumerate(
        tqdm(
            utils.io.get_batches(
                sentences, batch_size=batch_size, limit=limit, offset=offset
            ),
            desc=&#34;processing batches&#34;,
            total=len(sentences) // batch_size + 1,
        )
    ):
        sentence_features_filestem = f&#34;sentence-features_part{part:0&gt;4}&#34;
        token_features_filestem = f&#34;token-features_part{part:0&gt;4}&#34;

        ################################################################################
        #### LEXICAL FEATURES ##########################################################
        ################################################################################
        if process_lexical:
            utils.io.log(&#34;*** running lexical submodule pipeline&#34;)
            _ = lexical.utils.load_databases(features=&#34;all&#34;)

            lexical_features = utils.parallelize(
                lexical.get_features,
                sentence_batch,
                wrap_tqdm=True,
                desc=&#34;Lexical pipeline&#34;,
                max_workers=None if parallelize else 1,
            )

            lexical_out = output_dir / &#34;lexical&#34;
            lexical_out.mkdir(parents=True, exist_ok=True)
            utils.io.log(f&#34;outputting lexical token dataframe to {lexical_out}&#34;)

            # lexical is a special case since it returns dicts per token (rather than per sentence)
            # so we want to flatten it so that pandas creates a sensible dataframe from it.
            token_df = pd.DataFrame(chain.from_iterable(lexical_features))

            if output_format == &#34;tsv&#34;:
                token_df.to_csv(
                    lexical_out / f&#34;{token_features_filestem}.tsv&#34;, sep=&#34;\t&#34;, index=True
                )
                token_df.groupby(&#34;sentence&#34;).mean().to_csv(
                    lexical_out / f&#34;{sentence_features_filestem}.tsv&#34;,
                    sep=&#34;\t&#34;,
                    index=True,
                )
            elif output_format == &#34;pkl&#34;:
                token_df.to_pickle(
                    lexical_out / f&#34;{token_features_filestem}.pkl.gz&#34;, protocol=5
                )
                token_df.groupby(&#34;sentence&#34;).mean(numeric_only=True).to_pickle(
                    lexical_out / f&#34;{sentence_features_filestem}.pkl.gz&#34;, protocol=5
                )
            else:
                raise ValueError(f&#34;output format {output_format} not known&#34;)

            utils.io.log(&#34;--- finished lexical pipeline&#34;)

        ################################################################################
        #### SYNTAX FEATURES ###########################################################
        ################################################################################
        if process_syntax:
            utils.io.log(&#34;*** running syntax submodule pipeline&#34;)

            syntax_features = [
                syntax.get_features(
                    sentence,
                    dlt=True,
                    left_corner=True,
                    syntax_server=syntax_server,
                    syntax_port=syntax_port,
                )
                for i, sentence in enumerate(
                    tqdm(sentence_batch, desc=&#34;Syntax pipeline&#34;)
                )
            ]

            # put all features in the sentence df except the token-level ones
            token_syntax_features = {&#34;dlt&#34;, &#34;leftcorner&#34;}
            sentence_df = pd.DataFrame(
                [
                    {
                        k: v
                        for k, v in feature_dict.items()
                        if k not in token_syntax_features
                    }
                    for feature_dict in syntax_features
                ],
                index=[s.uid for s in sentence_batch],
            )

            # output gives us dataframes corresponding to each token-level feature. we need to combine these
            # into a single dataframe
            # we use functools.reduce to apply the pd.concat function to all the dataframes and join dataframes
            # that contain different features for the same tokens
            token_dfs = [
                reduce(
                    lambda x, y: pd.concat([x, y], axis=1, sort=False),
                    (v for k, v in feature_dict.items() if k in token_syntax_features),
                )
                for feature_dict in syntax_features
            ]

            for i, df in enumerate(token_dfs):
                token_dfs[i][&#34;index&#34;] = df.index
            #     token_dfs[i].reset_index(inplace=True)

            dicts = [
                {k: v[list(v.keys())[0]] for k, v in df.to_dict().items()}
                for df in token_dfs
            ]
            token_df = pd.DataFrame(dicts)
            token_df.index = token_df[&#34;index&#34;]
            # by this point we have merged dataframes with tokens along a column (rather than just a sentence)
            # now we need to stack them on top of each other to have all tokens across all sentences in a single dataframe
            # token_df = reduce(lambda x, y: pd.concat([x.reset_index(drop=True), y.reset_index(drop=True)]), token_dfs)
            # token_df = token_df.loc[:, ~token_df.columns.duplicated()]

            syntax_out = output_dir / &#34;syntax&#34;
            syntax_out.mkdir(parents=True, exist_ok=True)
            utils.io.log(f&#34;outputting syntax dataframes to {syntax_out}&#34;)

            if output_format == &#34;tsv&#34;:
                sentence_df.to_csv(
                    syntax_out / f&#34;{sentence_features_filestem}.tsv&#34;,
                    sep=&#34;\t&#34;,
                    index=True,
                )
                token_df.to_csv(
                    syntax_out / f&#34;{token_features_filestem}.tsv&#34;, sep=&#34;\t&#34;, index=True
                )
            elif output_format == &#34;pkl&#34;:
                sentence_df.to_pickle(
                    syntax_out / f&#34;{sentence_features_filestem}.pkl.gz&#34;, protocol=5
                )
                token_df.to_pickle(
                    syntax_out / f&#34;{token_features_filestem}.pkl.gz&#34;, protocol=5
                )
            else:
                raise ValueError(f&#34;unknown output format {output_format}&#34;)

            utils.io.log(&#34;--- finished syntax pipeline&#34;)

        # Calculate PMI
        # utils.GrabNGrams(sent_rows,pmi_paths)
        # utils.pPMI(sent_rows, pmi_paths)

        # Plot input data to benchmark data
        # utils.plot_usr_input_against_benchmark_dist_plots(df_benchmark, sent_embed)

    ################################################################################
    #### \end{run_sentence_features_pipeline} ######################################
    ################################################################################
    return output_dir</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#sentspace-002-c-2020-2022-evlab-mit-bcs-all-rights-reserved">Sentspace 0.0.2 (C) 2020-2022 EvLab, MIT BCS. All rights reserved.</a></li>
<li><a href="#sentspace">sentspace</a><ul>
<li><a href="#about">About</a></li>
<li><a href="#documentation">Documentation</a></li>
<li><a href="#usage">Usage</a><ul>
<li><a href="#1-cli">1. CLI</a></li>
<li><a href="#2-as-a-library">2. As a library</a></li>
</ul>
</li>
<li><a href="#installing">Installing</a><ul>
<li><a href="#1-install-using-uv">1. Install using uv</a></li>
<li><a href="#2-container-based-usage">2. Container-based usage</a></li>
<li><a href="#3-manual-install-use-as-last-resort">3. Manual install (use as last resort)</a></li>
</ul>
</li>
<li><a href="#submodules">Submodules</a><ul>
<li><a href="#lexical">lexical</a></li>
<li><a href="#contextual">contextual</a></li>
</ul>
</li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#whom-to-contact-for-help">Whom to contact for help</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="sentspace.Sentence" href="Sentence.html">sentspace.Sentence</a></code></li>
<li><code><a title="sentspace.lexical" href="lexical/index.html">sentspace.lexical</a></code></li>
<li><code><a title="sentspace.package_lexical" href="package_lexical.html">sentspace.package_lexical</a></code></li>
<li><code><a title="sentspace.syntax" href="syntax/index.html">sentspace.syntax</a></code></li>
<li><code><a title="sentspace.utils" href="utils/index.html">sentspace.utils</a></code></li>
<li><code><a title="sentspace.vis" href="vis/index.html">sentspace.vis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sentspace.run_sentence_features_pipeline" href="#sentspace.run_sentence_features_pipeline">run_sentence_features_pipeline</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>